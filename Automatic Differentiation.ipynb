{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12aed764",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Here, we will look at implementing automatic differentiation from scratch.\n",
    "\n",
    "We will build a computational graph structure, similar to what is utilized in tensorflow and pytorch. This will only involve numpy and base python.\n",
    "\n",
    "## Computational Graph\n",
    "\n",
    "Our computational graph will be implemented through the use of global variables (this was how tensorflow 1.x operated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76bbeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7182f80",
   "metadata": {},
   "source": [
    "### 1. Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3312255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.operators = set()\n",
    "        self.constants = set()\n",
    "        self.variables = set()\n",
    "        self.placeholders = set()\n",
    "        \n",
    "        # NOTE: our notation for global variables will be an underscore\n",
    "        global _g\n",
    "        # we are initializing our graph object as a global instance\n",
    "        _g = self\n",
    "        \n",
    "    def reset_counts(self, root):\n",
    "        if hasattr(root, 'count'):\n",
    "            root.count = 0\n",
    "        else:\n",
    "            for child in root.__subclasses__():\n",
    "                self.reset_counts(child)\n",
    "    \n",
    "    def reset_session(self):\n",
    "        try:\n",
    "            del _g\n",
    "        except:\n",
    "            pass\n",
    "        self.reset_counts(Node)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.reset_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448bfe1",
   "metadata": {},
   "source": [
    "### 2. Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b11199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da98d55",
   "metadata": {},
   "source": [
    "### 3. Placeholder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2b2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(Node):\n",
    "    # placeholder holds a node and awaits further inputs at computation\n",
    "    \n",
    "    count = 0\n",
    "    def __init__(self, name, dtype=float):\n",
    "        _g.placeholders.add(self)\n",
    "        self.value = None\n",
    "        self.gradient = None\n",
    "        self.name = f'Plc/{Placeholder.count}' if name is None else name\n",
    "        Placeholder.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Placeholder: name:{self.name}, value:{self.value}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f120eb",
   "metadata": {},
   "source": [
    "### 4. Constant class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5b30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    # constant node in computational graph\n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.constants.add(self)\n",
    "        self._value = value\n",
    "        self.gradient = None\n",
    "        self.name = f'Const/{Constant.count}' if name is None else name\n",
    "        Constant.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Constant: name:{self.name}, value:{self.value}'\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "    \n",
    "    @value.setter\n",
    "    def value(self):\n",
    "        raise VallueError('Cannot reassign constant')\n",
    "        self.value = None\n",
    "        self.gradient = None\n",
    "        self.name = f'Plc/{Placeholder.count}' if name is None else name\n",
    "        Placeholder.count += 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Placeholder: name:{self.name}, value:{self.value}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d143a",
   "metadata": {},
   "source": [
    "### 5. Variable Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8eb6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    # variable node, we track these for differentiation during graph computation\n",
    "    \n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.variables.add(self)\n",
    "        self.value = value\n",
    "        self.gradient = None\n",
    "        self.name = f'Var/{Variable.count}' if name is None else name\n",
    "        Variable.count += 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Variable: name:{self.name}, value:{self.value}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff058b3d",
   "metadata": {},
   "source": [
    "### 6. Operator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191b0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator(Node):\n",
    "    # An operator node in the computational graph.\n",
    "    \n",
    "    def __init__(self, name='Operator'):\n",
    "        _g.operators.add(self)\n",
    "        self.value = None\n",
    "        self.inputs = []\n",
    "        self.gradient = None\n",
    "        self.name = name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Operator: name:{self.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30176927",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "We will need operators to do the actual operations in our computational graph. We will define basic operators here, and they will subclass the operators node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfaa5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class add(Operator):\n",
    "    count = 0\n",
    "    # binary addition\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a,b]\n",
    "        self.name = f'add/{add.count}' if name is None else name\n",
    "        add.count += 1\n",
    "    \n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout, dout\n",
    "    \n",
    "class multiply(Operator):\n",
    "    count = 0\n",
    "    # binary multiplication\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs = [a,b]\n",
    "        self.name = f'mul/{multiply.count}' if name is None else name\n",
    "        multiply.count += 1\n",
    "    \n",
    "    def forward(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout*b, dout*a\n",
    "    \n",
    "class divide(Operator):\n",
    "    count = 0\n",
    "    # binary division\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs = [a,b]\n",
    "        self.name = f'div/{divide.count}' if name is None else name\n",
    "        divide.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a / b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout / b, dout * a / np.power(b,2)\n",
    "    \n",
    "class power(Operator):\n",
    "    count = 0\n",
    "    # raising to an exponent\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs = [a,b]\n",
    "        self.name = f'pow/{power.count}' if name is None else name\n",
    "        power.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return np.power(a, b)\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout * b * np.power(a, b-1), dout * np.log(a) * np.power(a, b)\n",
    "    \n",
    "class matmul(Operator):\n",
    "    count = 0\n",
    "    # matrix multiplication, defaults to binary multiplication\n",
    "    \n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs = [a, b]\n",
    "        self.name = f'matmul/{matmul.count}' if name is None else name\n",
    "        matmul.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a @ b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout @ b.T, a.T @ dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101e6d7",
   "metadata": {},
   "source": [
    "### Overload Operators Above\n",
    "\n",
    "We will now overload the operators defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36184859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will ensure that the operators we are comparing are graph nodes\n",
    "\n",
    "def node_wrapper(func, self, other):\n",
    "    if isinstance(other, Node):\n",
    "        return func(self, other)\n",
    "    if isinstance(other, float) or isinstance(other, int):\n",
    "        return func(self, Constant(other))\n",
    "    raise TypeError('Incompatible Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a92db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overload these operators\n",
    "\n",
    "Node.__add__ = lambda self, other: node_wrapper(add, self, other)\n",
    "Node.__mul__ = lambda self, other: node_wrapper(multiply, self, other)\n",
    "Node.__div__ = lambda self, other: node_wrapper(divide, self, other)\n",
    "Node.__neg__ = lambda self: node_wrapper(multiply, self, Constant(-1))\n",
    "Node.__pow__ = lambda self, other: node_wrapper(power, self, other)\n",
    "Node.__matmul__ = lambda self, other: node_wrapper(matmul, self, other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aca993",
   "metadata": {},
   "source": [
    "## Initialize a graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8918279",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Graph() as g:\n",
    "    x = Variable(1.0)\n",
    "    y = Variable(0.5)\n",
    "    \n",
    "    z = x*y + 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f2cacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Placeholder: name:Const/0, value:5}\n",
      "{Variable: name:Var/0, value:1.0, Variable: name:Var/1, value:0.5}\n",
      "{Operator: name:add/0, Operator: name:mul/0}\n"
     ]
    }
   ],
   "source": [
    "print(g.constants)\n",
    "print(g.variables)\n",
    "print(g.operators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae7931",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "We will now implement something similar to autograd from pytorch (or .apply_gradients from tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217ffedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(head_node=None, graph=_g):\n",
    "    # do a topological sort of all nodes\n",
    "    \n",
    "    vis = set()\n",
    "    ordering = []\n",
    "    \n",
    "    def _dfs(node):\n",
    "        if node not in vis:\n",
    "            vis.add(node)\n",
    "            if isinstance(node, Operator):\n",
    "                for input_node in node.inputs:\n",
    "                    _dfs(input_node)\n",
    "            ordering.append(node)\n",
    "            \n",
    "    if head_node is None:\n",
    "        for node in graph.operators:\n",
    "            _dfs(node)\n",
    "    else:\n",
    "        _dfs(head_node)\n",
    "    \n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6ea089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(order, feed_dict={}):\n",
    "    \n",
    "    for node in order:\n",
    "        if isinstance(node, Placeholder):\n",
    "            node.value = feed_dict[node.name]\n",
    "        \n",
    "        elif isinstance(node, Operator):\n",
    "            node.value = node.forward(*[prev_node.value for prev_node in node.inputs])\n",
    "            \n",
    "    return order[-1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ce28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(order, target_node=None):\n",
    "    \n",
    "    vis = set()\n",
    "    order[-1].gradient = 1\n",
    "    for node in reversed(order):\n",
    "        if isinstance(node, Operator):\n",
    "            inputs = node.inputs\n",
    "            grads = node.backward(*[x.value for x in inputs], dout=node.gradient)\n",
    "            for inp, grad in zip(inputs, grads):\n",
    "                if inp not in vis:\n",
    "                    inp.gradient = grad\n",
    "                else:\n",
    "                    inp.gradient += grad\n",
    "                vis.add(inp)\n",
    "    return [node.gradient for node in order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a4ae9",
   "metadata": {},
   "source": [
    "### Test out Feedforward\n",
    "\n",
    "We will be using the following function:\n",
    "\n",
    "$$f(x) = c_{1}(xy + c_{1}) + x$$\n",
    "\n",
    "We expect that the gradients will be:\n",
    "\n",
    "$$\\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\[3pt] \\frac{\\partial f}{\\partial y} \\\\[3pt] \\frac{\\partial f}{\\partial c_{1}} \\end{pmatrix} = \\begin{pmatrix} c_{1}y + 1 \\\\[3pt] c_{1}x \\\\[3pt] 2c_{1} + xy \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5bdddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Order:\n",
      "Placeholder: name:c1, value:1.3\n",
      "Variable: name:x, value:0.5\n",
      "Variable: name:y, value:0.6\n",
      "Operator: name:mul/0\n",
      "Operator: name:add/0\n",
      "Operator: name:mul/1\n",
      "Operator: name:add/1\n",
      "----------\n",
      "Forward pass expected: 2.58\n",
      "Forward pass computed: 2.58\n"
     ]
    }
   ],
   "source": [
    "val1, val2, val3 = 0.5, 0.6, 1.3\n",
    "\n",
    "with Graph() as g:\n",
    "    x = Variable(val1, name='x')\n",
    "    y = Variable(val2, name='y')\n",
    "    c1 = Constant(val3, name='c1')\n",
    "    \n",
    "    z = c1*(x*y + c1) + x\n",
    "    \n",
    "    # apply the topological sort to the outcome, then apply forward pass, backward pass to compute gradient\n",
    "    order = topological_sort(z)\n",
    "    res = forward_pass(order)\n",
    "    grads = backward_pass(order)\n",
    "    \n",
    "    print('Node Order:')\n",
    "    for node in order:\n",
    "        print(node)\n",
    "        \n",
    "    print('-'*10)\n",
    "    print(f'Forward pass expected: {(val1*val2 + val3)*val3 + val1}')\n",
    "    print(f'Forward pass computed: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c44b4",
   "metadata": {},
   "source": [
    "### Test out Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532ab0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx expected = 1.78\n",
      "dz/dx computed = 1.78\n",
      "dz/dy expected = 0.65\n",
      "dz/dy computed = 0.65\n",
      "dz/dc expected = 2.9\n",
      "dz/dc computed = 2.9000000000000004\n"
     ]
    }
   ],
   "source": [
    "dz_dx = [a for a in order if a.name=='x'][0]\n",
    "dz_dy = [a for a in order if a.name=='y'][0]\n",
    "dz_dc = [a for a in order if a.name=='c1'][0]\n",
    "\n",
    "print(f\"dz/dx expected = {val3*val2+1}\")\n",
    "print(f\"dz/dx computed = {dz_dx.gradient}\")\n",
    "\n",
    "print(f\"dz/dy expected = {val1*val3}\")\n",
    "print(f\"dz/dy computed = {dz_dy.gradient}\")\n",
    "\n",
    "print(f\"dz/dc expected = {val1*val2+2*val3}\")\n",
    "print(f\"dz/dc computed = {dz_dc.gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c75c6",
   "metadata": {},
   "source": [
    "So our gradients work as expected (with a little bit of round off error). Let's see if this works on arbitrary tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d830f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Order:\n",
      "Variable: name:T, value:[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "Placeholder: name:Const/1, value:3\n",
      "Operator: name:pow/0\n",
      "----------\n",
      "Forward pass expected: [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "Forward pass computed: [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "T = np.ones((5,5))\n",
    "U = np.ones((5,5))\n",
    "c = 6.0\n",
    "with Graph() as g:\n",
    "    x = Variable(T, name='T')\n",
    "    y = Variable(U, name='U')\n",
    "    z = Constant(6.0, name='c')\n",
    "    \n",
    "    # define function to differentiate\n",
    "    h = x**3\n",
    "    \n",
    "    order2 = topological_sort(h)\n",
    "    result2 = forward_pass(order2)\n",
    "    gradients2 = backward_pass(order2)\n",
    "    \n",
    "    print('Node Order:')\n",
    "    for node in order2:\n",
    "        print(node)\n",
    "        \n",
    "    print('-'*10)\n",
    "    print(f'Forward pass expected: {T**3}')\n",
    "    print(f'Forward pass computed: {result2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b03a4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh_dT expected = [[3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n",
      "dh_dT computed = [[3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "dh_dT = [a for a in order2 if a.name == 'T'][0]\n",
    "\n",
    "print(f'dh_dT expected = {3*(T**2)}')\n",
    "print(f'dh_dT computed = {dh_dT.gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875f14",
   "metadata": {},
   "source": [
    "So this automatic gradient works for polynomial-like functions of arbitrary tensors!\n",
    "\n",
    "One thing we need to work on is that we should define derivatives for functions like $\\sin$, $\\cos$, $\\exp$, $\\log$ so that we can cover all bases for elementary functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blank_env",
   "language": "python",
   "name": "blank_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
